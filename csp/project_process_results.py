"""
Process Sudoku Results CSV and Generate Metrics Report

This script processes the results CSV file generated by project_csp.py
and generates a comprehensive metrics report without re-running the solver.
"""

import csv
import statistics
import argparse
import sys
import os
import glob
from io import StringIO


def get_difficulty_category(difficulty_str):
    """
    Map difficulty value to category.
    Easy = 0-1, Medium = 1-3, Hard = 3+
    
    Args:
        difficulty_str: String representation of difficulty value
    
    Returns:
        'Easy', 'Medium', 'Hard', or None if invalid
    """
    try:
        diff = float(difficulty_str)
        if 0.0 <= diff < 1.0:
            return 'Easy'
        elif 1.0 <= diff < 3.0:
            return 'Medium'
        elif 3.0 <= diff:
            return 'Hard'
    except (ValueError, TypeError):
        pass
    return None


def process_metrics(results_csv_path):
    """
    Process the results CSV and compute timing statistics by difficulty.
    
    Args:
        results_csv_path: Path to the results CSV file
    
    Returns:
        Dictionary with timing statistics grouped by difficulty category
    """
    # Group solve times by difficulty category
    difficulty_times = {
        'Easy': [],
        'Medium': [],
        'Hard': []
    }
    
    overall_stats = {
        'total': 0,
        'solved': 0,
        'failed': 0,
        'correct': 0,
        'incorrect': 0,
        'all_times': []
    }
    
    with open(results_csv_path, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            overall_stats['total'] += 1
            
            computed_solution = row.get('computed_solution', '')
            expected_solution = row.get('solution', '')
            difficulty = row.get('difficulty', '')
            solve_time_str = row.get('solve_time', '')
            
            # Process solve times for successfully solved puzzles
            if computed_solution and solve_time_str:
                try:
                    solve_time = float(solve_time_str)
                    overall_stats['solved'] += 1
                    overall_stats['all_times'].append(solve_time)
                    
                    # Verify correctness
                    if computed_solution == expected_solution:
                        overall_stats['correct'] += 1
                    else:
                        overall_stats['incorrect'] += 1
                    
                    # Group by difficulty category
                    category = get_difficulty_category(difficulty)
                    if category and category in difficulty_times:
                        difficulty_times[category].append(solve_time)
                except ValueError:
                    pass
            else:
                overall_stats['failed'] += 1
    
    # Compute statistics for each difficulty
    metrics = {}
    for category in ['Easy', 'Medium', 'Hard']:
        times = difficulty_times[category]
        if times:
            metrics[category] = {
                'Minimum': min(times),
                'Median': statistics.median(times),
                'Mean': statistics.mean(times),
                'Maximum': max(times),
                'count': len(times)
            }
        else:
            metrics[category] = {
                'Minimum': 0.0,
                'Median': 0.0,
                'Mean': 0.0,
                'Maximum': 0.0,
                'count': 0
            }
    
    return metrics, overall_stats


def print_metrics_table(metrics, algorithm_name='Backtracking'):
    """
    Print CPU Time statistics in table format matching the reference format.
    
    Args:
        metrics: Dictionary with timing statistics from process_metrics()
        algorithm_name: Name of the algorithm/solver to display
    """
    print("\n" + "="*60)
    print("CPU Time (sec)")
    print("="*60)
    print(f"{'':<20} {algorithm_name:<15}")
    print("-"*60)
    
    # Print statistics for each difficulty level
    for category in ['Easy', 'Medium', 'Hard']:
        stats = metrics[category]
        
        # Print difficulty header
        print(f"{category}:")
        
        # Print each metric
        print(f"  {'Minimum:':<18} {stats['Minimum']:<15.6f}")
        print(f"  {'Median:':<18} {stats['Median']:<15.6f}")
        print(f"  {'Mean:':<18} {stats['Mean']:<15.6f}")
        print(f"  {'Maximum:':<18} {stats['Maximum']:<15.6f}")
        
        if category != 'Hard':  # Don't print separator after last category
            print("-"*60)
    
    print("="*60)
    print(f"\nNote: Statistics based on {sum(metrics[c]['count'] for c in metrics)} successfully solved puzzles")
    for category in ['Easy', 'Medium', 'Hard']:
        count = metrics[category]['count']
        if count > 0:
            print(f"  {category}: {count} puzzles")


def print_overall_stats(overall_stats):
    """
    Print overall statistics summary.
    
    Args:
        overall_stats: Dictionary with overall statistics
    """
    print("\n" + "="*60)
    print("Overall Statistics")
    print("="*60)
    print(f"Total puzzles processed:     {overall_stats['total']:,}")
    print(f"Successfully solved:        {overall_stats['solved']:,} ({overall_stats['solved']/overall_stats['total']*100:.2f}%)")
    print(f"Failed to solve:            {overall_stats['failed']:,} ({overall_stats['failed']/overall_stats['total']*100:.2f}%)")
    print(f"Correct solutions:          {overall_stats['correct']:,} ({overall_stats['correct']/overall_stats['total']*100:.2f}%)")
    print(f"Incorrect solutions:        {overall_stats['incorrect']:,} ({overall_stats['incorrect']/overall_stats['total']*100:.2f}%)")
    
    if overall_stats['solved'] > 0:
        accuracy = overall_stats['correct'] / overall_stats['solved'] * 100
        print(f"Solution accuracy:          {accuracy:.2f}%")
    
    # Overall timing statistics
    if overall_stats['all_times']:
        times = overall_stats['all_times']
        print("\n" + "-"*60)
        print("Overall Timing Statistics (seconds)")
        print("-"*60)
        print(f"Mean solve time:            {statistics.mean(times):.6f}")
        print(f"Median solve time:          {statistics.median(times):.6f}")
        print(f"Min solve time:             {min(times):.6f}")
        print(f"Max solve time:             {max(times):.6f}")
        if len(times) > 1:
            print(f"Standard deviation:         {statistics.stdev(times):.6f}")
        
        # Percentiles
        sorted_times = sorted(times)
        n = len(sorted_times)
        print(f"\nPercentiles:")
        print(f"  25th percentile (Q1):      {sorted_times[n//4]:.6f}")
        print(f"  50th percentile (median):  {sorted_times[n//2]:.6f}")
        print(f"  75th percentile (Q3):     {sorted_times[3*n//4]:.6f}")
        print(f"  90th percentile:           {sorted_times[int(0.9*n)]:.6f}")
        print(f"  95th percentile:           {sorted_times[int(0.95*n)]:.6f}")
        print(f"  99th percentile:           {sorted_times[int(0.99*n)]:.6f}")


def extract_algorithm_name(filename):
    """
    Extract algorithm/solver name from filename.
    Examples:
        sudoku_test_set_random_10k_fc_results_20251109_205726.csv -> FC
        sudoku_test_set_random_10k_mac_results_20251109_205757.csv -> MAC
        sudoku_test_set_random_10k_glucose3_results_20251109_211323.csv -> Glucose3
        sudoku_test_set_random_10k_porplogic_results_20251109_211323.csv -> Propositional Logic
    
    Args:
        filename: Name of the CSV file
    
    Returns:
        Algorithm/solver name (capitalized)
    """
    base_name = os.path.basename(filename)
    
    # Remove .csv extension
    if base_name.endswith('.csv'):
        base_name = base_name[:-4]
    
    # Look for common patterns: _fc_, _mac_, _glucose3_, _cadical_, etc.
    # Pattern is typically: ..._method_results_TIMESTAMP
    if '_fc_results_' in base_name:
        return 'FC (Forward Checking)'
    elif '_mac_results_' in base_name:
        return 'MAC (Maintaining Arc Consistency)'
    elif '_glucose3_results_' in base_name or '_glucose_results_' in base_name:
        return 'Glucose3'
    elif '_cadical_results_' in base_name or '_cadical153_results_' in base_name:
        return 'CaDiCaL'
    elif '_minisat_results_' in base_name or '_minisat22_results_' in base_name:
        return 'MiniSAT'
    elif '_lingeling_results_' in base_name:
        return 'Lingeling'
    elif '_porplogic_results_' in base_name or '_propositional_logic_results_' in base_name:
        return 'Propositional Logic (SAT)'
    else:
        # Try to extract method name from pattern: ..._method_results_...
        parts = base_name.split('_results_')
        if len(parts) > 1:
            # Get the part before _results_ and extract the last segment
            before_results = parts[0]
            segments = before_results.split('_')
            if len(segments) > 0:
                method = segments[-1]
                return method.upper() if len(method) <= 3 else method.capitalize()
        return 'Backtracking'


def process_single_file(results_csv_path, summary_only=False, no_save=False):
    """
    Process a single results CSV file and generate a report.
    
    Args:
        results_csv_path: Path to the results CSV file
        summary_only: If True, show only the CPU Time table
        no_save: If True, do not save output to text file
    """
    # Generate output text file path (same name as CSV but with .txt extension)
    if results_csv_path.endswith('.csv'):
        output_txt = results_csv_path[:-4] + '.txt'
    else:
        output_txt = results_csv_path + '.txt'
    
    # Extract algorithm name from filename
    algorithm_name = extract_algorithm_name(results_csv_path)
    
    # Use StringIO to capture all output
    output_buffer = StringIO()
    original_stdout = sys.stdout
    
    # Create a Tee-like class that writes to both stdout and buffer
    class Tee:
        def __init__(self, *files):
            self.files = files
        
        def write(self, obj):
            for f in self.files:
                f.write(obj)
                f.flush()
        
        def flush(self):
            for f in self.files:
                f.flush()
    
    # Redirect stdout to both console and buffer
    sys.stdout = Tee(original_stdout, output_buffer)
    
    try:
        print(f"\n{'='*60}")
        print(f"Processing results from: {results_csv_path}")
        print("Computing metrics...")
        
        metrics, overall_stats = process_metrics(results_csv_path)
        
        # Print CPU Time table
        print_metrics_table(metrics, algorithm_name)
        
        # Print overall statistics unless summary-only flag is set
        if not summary_only:
            print_overall_stats(overall_stats)
        
        print("\n" + "="*60)
        
    finally:
        # Restore original stdout
        sys.stdout = original_stdout
    
    # Save to text file unless --no-save flag is set
    if not no_save:
        try:
            with open(output_txt, 'w') as f:
                f.write(output_buffer.getvalue())
            print(f"\nReport saved to: {output_txt}")
        except Exception as e:
            print(f"\nWarning: Could not save to {output_txt}: {e}", file=sys.stderr)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Process Sudoku results CSV and generate metrics report'
    )
    parser.add_argument(
        'results_csv',
        nargs='?',
        default=None,
        help='Path to a specific results CSV file (optional). If not provided, processes all CSV files in the results directory.'
    )
    parser.add_argument(
        '--results-dir',
        default='results',
        help='Directory containing results CSV files (default: results)'
    )
    parser.add_argument(
        '--summary-only',
        action='store_true',
        help='Show only the CPU Time table (skip overall statistics)'
    )
    parser.add_argument(
        '--no-save',
        action='store_true',
        help='Do not save output to text file (only print to stdout)'
    )
    
    args = parser.parse_args()
    
    # Determine which files to process
    if args.results_csv:
        # Process single file
        if not os.path.exists(args.results_csv):
            print(f"Error: File not found: {args.results_csv}", file=sys.stderr)
            sys.exit(1)
        process_single_file(args.results_csv, args.summary_only, args.no_save)
    else:
        # Process all CSV files in results directory
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)  # Go up one level from csp/
        results_dir = os.path.join(project_root, args.results_dir)
        
        if not os.path.exists(results_dir):
            print(f"Error: Results directory not found: {results_dir}", file=sys.stderr)
            sys.exit(1)
        
        # Find all CSV files in results directory
        csv_pattern = os.path.join(results_dir, '*.csv')
        csv_files = sorted(glob.glob(csv_pattern))
        
        if not csv_files:
            print(f"No CSV files found in {results_dir}", file=sys.stderr)
            sys.exit(1)
        
        print(f"Found {len(csv_files)} CSV file(s) in {results_dir}")
        print("Processing each file separately...\n")
        
        # Process each CSV file
        for csv_file in csv_files:
            process_single_file(csv_file, args.summary_only, args.no_save)
        
        print(f"\n{'='*60}")
        print(f"Completed processing {len(csv_files)} file(s)")
        print("="*60)

